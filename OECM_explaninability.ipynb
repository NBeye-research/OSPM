{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hila-chefer/Transformer-Explainability/blob/main/Transformer_explainability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj6EnzRyAY5q"
      },
      "source": [
        "# **Transformer Interpretability Beyond Attention Visualization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdJ4YOiTBtAz"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "TtqMdXdTEKAP"
      },
      "outputs": [],
      "source": [
        "#@title Imagenet class indices to names\n",
        "#%%capture\n",
        "CLS2IDX = {0: 'benign',1:'malignant',2:'premalignant'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IDKqZyCG3rmS"
      },
      "outputs": [],
      "source": [
        "from ViT.ViT_LRP import vit_large_patch16_224 as vit_LRP\n",
        "from ViT.ViT_explanation_generator import LRP\n",
        "#@title Perform thresholding on the relevance (using Otsu's method)\n",
        "use_thresholding =  False#@param {type:\"boolean\"}\n",
        "# initialize ViT pretrained\n",
        "model = vit_LRP(pretrained=False, num_classes=3).cuda()\n",
        "\n",
        "# replace your finetuned model\n",
        "model_dir='./models/vit_large_patch16_224_model_best.pth'\n",
        "\n",
        "checkpoint = torch.load(model_dir)['model']\n",
        "new_model_dict = {k.replace('module.', ''): v for k, v in checkpoint.items() }\n",
        "msg = model.load_state_dict(new_model_dict)\n",
        "model.cuda()\n",
        "model.eval()\n",
        "attribution_generator = LRP(model)\n",
        "\n",
        "print(msg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtHosD9lCgAA"
      },
      "outputs": [],
      "source": [
        "import PIL\n",
        "import torch.nn as nn\n",
        "import codecs\n",
        "\n",
        "normalize = transforms.Normalize(mean = [0.50351185, 0.30116007, 0.20442231], std = [0.2821921, 0.22173707, 0.17406568])\n",
        "transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224), interpolation=PIL.Image.BICUBIC),\n",
        "        # transforms.CenterCrop((image_size,image_size)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ])\n",
        "\n",
        "# create heatmap from mask on image\n",
        "def show_cam_on_image(img, mask):\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
        "    \n",
        "    heatmap = np.float32(heatmap) / 255\n",
        "    cam = heatmap + np.float32(img)\n",
        "    \n",
        "    cam = cam / np.max(cam)\n",
        "    return cam\n",
        "\n",
        "\n",
        "def generate_visualization(original_image, class_index=None):\n",
        "    transformer_attribution = attribution_generator.generate_LRP(original_image.unsqueeze(0).cuda(), method=\"transformer_attribution\", index=class_index).detach()\n",
        "    transformer_attribution = transformer_attribution.reshape(1, 1, 14, 14)\n",
        "    transformer_attribution = torch.nn.functional.interpolate(transformer_attribution, scale_factor=16, mode='bilinear')\n",
        "    transformer_attribution = transformer_attribution.reshape(224, 224).data.cpu().numpy()\n",
        "    transformer_attribution = (transformer_attribution - transformer_attribution.min()) / (transformer_attribution.max() - transformer_attribution.min())\n",
        "\n",
        "    if use_thresholding:\n",
        "      transformer_attribution = transformer_attribution * 255\n",
        "      transformer_attribution = transformer_attribution.astype(np.uint8)\n",
        "      ret, transformer_attribution = cv2.threshold(transformer_attribution, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "      transformer_attribution[transformer_attribution == 255] = 1\n",
        "\n",
        "    image_transformer_attribution = original_image.permute(1, 2, 0).data.cpu().numpy()\n",
        "    image_transformer_attribution = (image_transformer_attribution - image_transformer_attribution.min()) / (image_transformer_attribution.max() - image_transformer_attribution.min())\n",
        "    vis = show_cam_on_image(image_transformer_attribution, transformer_attribution)\n",
        "    vis =   vis\n",
        "    vis =  np.uint8(vis*255)\n",
        "    vis = cv2.cvtColor(np.array(vis), cv2.COLOR_RGB2BGR)\n",
        "    return vis\n",
        "\n",
        "def print_top_classes(predictions, **kwargs):\n",
        "    \n",
        "    # Print Top-5 predictions\n",
        "    prob = torch.softmax(predictions, dim=1)\n",
        "    class_indices = predictions.data.topk(3, dim=1)[1][0].tolist()\n",
        "    max_str_len = 0\n",
        "    class_names = []\n",
        "    for cls_idx in class_indices:\n",
        "        class_names.append(CLS2IDX[cls_idx])\n",
        "        if len(CLS2IDX[cls_idx]) > max_str_len:\n",
        "            max_str_len = len(CLS2IDX[cls_idx])\n",
        "    \n",
        "    print('Top 3 classes:')\n",
        "    for cls_idx in class_indices:\n",
        "        output_string = '\\t{} : {}'.format(cls_idx, CLS2IDX[cls_idx])\n",
        "        output_string += ' ' * (max_str_len - len(CLS2IDX[cls_idx])) + '\\t\\t'\n",
        "        output_string += 'value = {:.3f}\\t prob = {:.1f}%'.format(predictions[0, cls_idx], 100 * prob[0, cls_idx])\n",
        "        print(output_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "ZPbx6CIHEl08",
        "outputId": "d42b5ff4-8206-4588-971f-ac7d0ed6fa89"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "root_dir = './example_img/'\n",
        "image_name = 'example_oremalignant.jpg'\n",
        "name, _ = os.path.splitext(image_name)\n",
        "\n",
        "image = Image.open(root_dir + image_name)\n",
        "resize_image = image.resize((512,512),Image.LANCZOS)\n",
        "transform_image = transform(image)\n",
        "use_thresholding = False\n",
        "\n",
        "# fig, axs = plt.subplots(1, 4)\n",
        "fig, axs = plt.subplots(1,2,dpi=200)\n",
        "axs[0].imshow(resize_image)\n",
        "axs[0].axis('off')\n",
        "\n",
        "output = model(transform_image.unsqueeze(0).cuda())\n",
        "prob = torch.softmax(output, dim=1)\n",
        "\n",
        "print_top_classes(output)\n",
        "\n",
        "# the predicted class\n",
        "cate1 = generate_visualization(transform_image)\n",
        "resize_image.save(root_dir + name + '_resized.tiff') \n",
        "plt.imsave(root_dir + name + '_热图.tiff', cate1)\n",
        "\n",
        "axs[1].imshow(cate1)\n",
        "axs[1].axis('off')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOZAljZCH9K62jPH5tqgQlf",
      "include_colab_link": true,
      "name": "Transformer-explainability.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pytorch1.10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
